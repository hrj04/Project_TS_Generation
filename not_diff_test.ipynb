{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harim/Desktop/pyproject/Project_TS_Generation/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from torchmetrics import MeanAbsolutePercentageError\n",
    "\n",
    "\n",
    "from data.dataloader import dataloader_info\n",
    "from utils.utils import load_yaml_config, instantiate_from_config\n",
    "\n",
    "from models.predictor import GRU\n",
    "from models.solver import Trainer\n",
    "from data.dataloader import dataloader_info\n",
    "from utils.visualize import visualize_pca, visualize_tsne, visualize_kernel\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configurations\n",
    "configs = load_yaml_config(\"configs/stock_diff2.yaml\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Initialize Diffusion_TS Model\n",
    "diffusion_ts = instantiate_from_config(configs['model']).to(device)\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "dl_info = dataloader_info(configs)\n",
    "dl_info_test = dataloader_info(configs, train=False)\n",
    "dl_info_test[\"dataloader\"]\n",
    "dl_info_test[\"dataset\"]\n",
    "\n",
    "dataset = dl_info['dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = dl_info[\"dataloader\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(config_solver=configs[\"solver\"], model=diffusion_ts, dataloader=dl_info[\"dataloader\"])\n",
    "trainer.train_decomp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate adversarial data\n",
    "fake_data = diffusion_ts.generate_mts(batch_size=6000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load original data\n",
    "seq_length, feature_dim = dataset.window, dataset.feature_dim\n",
    "gt_data = np.load(os.path.join(dataset.dir, f\"stock_origin_data_{seq_length}_train.npy\"))\n",
    "idx = np.random.permutation(len(gt_data))[:3000]\n",
    "ori_data = gt_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pca(ori_data, fake_data, 3000)\n",
    "visualize_tsne(ori_data, fake_data, 3000)\n",
    "visualize_kernel(ori_data, fake_data, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_norm_origin = torch.from_numpy(np.load(os.path.join(dataset.dir, f\"stock_origin_data_{seq_length}_test.npy\"))).to(device)\n",
    "test_mean = torch.from_numpy(np.load(os.path.join(dataset.dir, f\"stock_origin_mean_{seq_length}_test.npy\"))).to(device)\n",
    "test_std = torch.from_numpy(np.load(os.path.join(dataset.dir, f\"stock_origin_std_{seq_length}_test.npy\"))).to(device)\n",
    "\n",
    "test_dataset = TensorDataset(test_data_norm_origin, test_mean, test_std)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "lossfn = nn.L1Loss()\n",
    "# lossfn = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=100, description=\"\"):\n",
    "    model.train()\n",
    "    with tqdm(range(num_epochs), total=num_epochs) as pbar:\n",
    "        for e in pbar:\n",
    "            for data in dataloader:\n",
    "                x_train = data[:,:-1,:].float().to(device)\n",
    "                y_train = data[:,-1:,0].float().to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(x_train)\n",
    "                loss = criterion(outputs, y_train)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            pbar.set_description(f\"{description} loss: {loss.item():.6f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    # define loss for comparison\n",
    "    l1loss = nn.L1Loss()\n",
    "    l2loss = nn.MSELoss()\n",
    "    mapeloss = MeanAbsolutePercentageError().to(device)\n",
    "    \n",
    "    total_l1 = 0\n",
    "    total_l2 = 0\n",
    "    total_mape = 0\n",
    "\n",
    "    predictions, true_vals = [], []\n",
    "    with torch.no_grad():\n",
    "        for data_norm, data_mean, data_std  in dataloader:\n",
    "            x_test = data_norm[:, :(seq_length - 1), :].float().to(device)\n",
    "            y_test = data_norm[:, (seq_length - 1):, :1].float().to(device)\n",
    "            y_pred = model(x_test).view(-1,1,1)\n",
    "            \n",
    "            y_pred_unnorm = y_pred * data_std[:, :, :1] + data_mean[:, :, :1]\n",
    "            y_test_unnorm = y_test * data_std[:, :, :1] + data_mean[:, :, :1]\n",
    "            total_l1 += l1loss(y_pred_unnorm, y_test_unnorm) * len(data_norm)\n",
    "            total_l2 += l2loss(y_pred_unnorm, y_test_unnorm) * len(data_norm)\n",
    "            # total_mape += mapeloss(y_pred_unnorm, y_test_unnorm).item() * len(data_norm)\n",
    "\n",
    "            predictions.append(y_pred_unnorm.cpu().numpy())\n",
    "            true_vals.append(y_test_unnorm.cpu().numpy())\n",
    "\n",
    "    n_data = len(dataloader.dataset)\n",
    "    total_l1 /= n_data\n",
    "    total_l2 /= n_data\n",
    "    # total_mape /= n_data\n",
    "    \n",
    "    predictions = np.concatenate(predictions).squeeze()\n",
    "    true_vals = np.concatenate(true_vals).squeeze()\n",
    "    mape_loss = mapeloss(torch.tensor(predictions), torch.tensor(true_vals)).item()\n",
    "    \n",
    "    return total_l1, total_l2, mape_loss, predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " loss: 0.125789:   7%|â–‹         | 201/3000 [00:04<01:07, 41.41it/s]"
     ]
    }
   ],
   "source": [
    "gt_predictor = GRU(5, 50, 1, 2).to(device)\n",
    "gt_optimizer = Adam(gt_predictor.parameters(), lr=0.001)\n",
    "\n",
    "train_model(gt_predictor, dl_info[\"dataloader\"],lossfn, gt_optimizer, num_epochs=3000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_norm_origin = torch.from_numpy(np.load(os.path.join(dataset.dir, f\"stock_origin_data_{seq_length}_train.npy\"))).to(device)\n",
    "train_mean = torch.from_numpy(np.load(os.path.join(dataset.dir, f\"stock_origin_mean_{seq_length}_train.npy\"))).to(device)\n",
    "train_std = torch.from_numpy(np.load(os.path.join(dataset.dir, f\"stock_origin_std_{seq_length}_train.npy\"))).to(device)\n",
    "\n",
    "train_dataset = TensorDataset(train_data_norm_origin, train_mean, train_std)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "lossfn = nn.L1Loss()\n",
    "# lossfn = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1, l2, mape, pre_y, true_y = evaluate_model(gt_predictor, train_loader)\n",
    "print(f\"Adv_synthetic : L1 loss: {l1:0.5f} \\t L2 Loss : {l2:0.5f} \\t MAPE loss : {mape:0.5f} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pre_y[-200:])\n",
    "plt.plot(true_y[-200:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1, l2, mape, pre_y, true_y = evaluate_model(gt_predictor, test_loader)\n",
    "print(f\"Adv_synthetic : L1 loss: {l1:0.5f} \\t L2 Loss : {l2:0.5f} \\t MAPE loss : {mape:0.5f} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = true_y[-200:]-pre_y[-200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(a, marker=\"o\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pre_y[-200:], marker=\"o\")\n",
    "plt.plot(true_y[-200:], marker=\"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_data = np.concatenate([gt_data, fake_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_data =torch.from_numpy(syn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_dl = DataLoader(syn_data, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(gt_predictor, syn_dl,lossfn,gt_optimizer, 3000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1, l2, mape, pre_y, true_y = evaluate_model(gt_predictor, test_loader)\n",
    "print(f\"Adv_synthetic : L1 loss: {l1:0.5f} \\t L2 Loss : {l2:0.5f} \\t MAPE loss : {mape:0.5f} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adv_synthetic : L1 loss: 0.81854 \t L2 Loss : 1.96458 \t MAPE loss : 0.00976 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pre_y[-200:], marker=\"o\")\n",
    "plt.plot(true_y[-200:], marker=\"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_data.mean(2).reshape(-1,).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "\n",
    "stat, p_value = stats.shapiro(ori_data.mean(2).reshape(-1,))\n",
    "print(f'Statistic: {stat}, p-value: {p_value}')\n",
    "if p_value > 0.05:\n",
    "    print('The data is normally distributed (fail to reject H0)')\n",
    "else:\n",
    "    print('The data is not normally distributed (reject H0)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "stat, p_value = stats.kstest(ori_data.mean(2).reshape(-1,), 'norm')\n",
    "print(f'Statistic: {stat}, p-value: {p_value}')\n",
    "if p_value > 0.05:\n",
    "    print('The data is normally distributed (fail to reject H0)')\n",
    "else:\n",
    "    print('The data is not normally distributed (reject H0)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "stats.probplot(ori_data.mean(2).reshape(-1,), dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
