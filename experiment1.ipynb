{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment objective\n",
    "Is differencing technic effective?\n",
    "\n",
    "## Experiment setting\n",
    "compare the result with differencing and without differencing\n",
    "### Dataset\n",
    "train : AAPL, MSFT, NVDA, AMZN, COST stock close price (2000-01-01~2013-12-31)  \n",
    "test : AAPL, MSFT, NVDA, AMZN, COST stock close price (2014-01-01~2023-12-31)\n",
    "\n",
    "### Scenario1\n",
    "forecasting one-step ahead AAPL stock close price based on the past 23 steps on itself and 4 other stocks close price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harim/Desktop/pyproject/Project_TS_Generation/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from torchmetrics import MeanAbsolutePercentageError\n",
    "\n",
    "from data.dataloader import dataloader_info\n",
    "from utils.utils import load_yaml_config, instantiate_from_config\n",
    "\n",
    "from models.predictor import GRU\n",
    "from data.dataloader import dataloader_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Differencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, num_epochs, description, device):\n",
    "    model.train()\n",
    "    with tqdm(range(num_epochs), total=num_epochs) as pbar:\n",
    "        for _ in pbar:\n",
    "            for data_diff, *_ in dataloader:\n",
    "                x_train = data_diff[:,:-1,:].float().to(device)\n",
    "                y_train = data_diff[:,-1:,0].float().to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(x_train)\n",
    "                loss = criterion(outputs, y_train)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            pbar.set_description(f\"{description} loss: {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    l1loss = nn.L1Loss()\n",
    "    l2loss = nn.MSELoss()\n",
    "    mapeloss = MeanAbsolutePercentageError().to(device)\n",
    "    \n",
    "    total_l1 = 0\n",
    "    total_l2 = 0\n",
    "    predictions, ground_truth = [], []\n",
    "    with torch.no_grad():\n",
    "        for data_diff, data_norm, data_mean, data_std  in dataloader:\n",
    "            data_norm = data_norm.to(device)\n",
    "            data_diff = data_diff.to(device)\n",
    "            data_mean = data_mean.to(device)\n",
    "            data_std = data_std.to(device)\n",
    "            batch_size = len(data_diff)\n",
    "            x_diff = data_diff[:, :-1, :].float()\n",
    "            y_true_diff = data_diff[:, -1:, :1].float()\n",
    "            y_pred_diff = model(x_diff).view(-1,1,1)\n",
    "            y_pred_norm = data_norm[:,-2:-1,:1] + y_pred_diff\n",
    "            y_true_norm = data_norm[:,-2:-1,:1] + y_true_diff\n",
    "            \n",
    "            y_pred_unnorm = y_pred_norm * data_std[:, :, :1] + data_mean[:, :, :1]\n",
    "            y_test_unnorm = y_true_norm * data_std[:, :, :1] + data_mean[:, :, :1]\n",
    "            \n",
    "            total_l1 += l1loss(y_pred_unnorm, y_test_unnorm) * batch_size\n",
    "            total_l2 += l2loss(y_pred_unnorm, y_test_unnorm) * batch_size\n",
    "\n",
    "            predictions.append(y_pred_unnorm.cpu().numpy())\n",
    "            ground_truth.append(y_test_unnorm.cpu().numpy())\n",
    "\n",
    "    n_data = len(dataloader.dataset)\n",
    "    total_l1 /= n_data\n",
    "    total_l2 /= n_data\n",
    "    predictions = np.concatenate(predictions).squeeze()\n",
    "    ground_truth = np.concatenate(ground_truth).squeeze()\n",
    "    mape_loss = mapeloss(torch.tensor(predictions), torch.tensor(ground_truth)).item()\n",
    "    \n",
    "    return total_l1.item(), total_l2.item(), mape_loss, predictions, ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configurations\n",
    "configs = load_yaml_config(\"configs/experiments1_w_diff.yaml\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Initialize Diffusion_TS Model\n",
    "diffusion_ts = instantiate_from_config(configs['model']).to(device)\n",
    "diffusion_ts.load_state_dict(torch.load(\"check_points/experiments1_w_diff/DiffusionTS_5000.pth\"))\n",
    "\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataloader, dataset\n",
    "dl_info_train = dataloader_info(configs, train=True)\n",
    "dl_info_test = dataloader_info(configs, train=False)\n",
    "\n",
    "dl_train = dl_info_train[\"dataloader\"]\n",
    "ds_train = dl_info_train[\"dataset\"]\n",
    "\n",
    "dl_test = dl_info_test[\"dataloader\"]\n",
    "ds_test = dl_info_test[\"dataset\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. pre-training for baseline predictor\n",
    "\n",
    "predictor_base = GRU(input_dim=5, \n",
    "                   hidden_dim=50, \n",
    "                   output_dim=1, \n",
    "                   num_layers=2).to(device)\n",
    "optimizer_base = Adam(predictor_base.parameters(), lr=0.001)\n",
    "lossfn = nn.L1Loss()\n",
    "\n",
    "train_model(predictor_base, \n",
    "            dl_train, \n",
    "            lossfn, \n",
    "            optimizer_base, \n",
    "            num_epochs=3000, \n",
    "            description=\"Baseline\",\n",
    "            device=device)\n",
    "\n",
    "l1, l2, mape, pred_y, true_y = evaluate_model(predictor_base, dl_test, device=device)\n",
    "print(f\"Baseline : L1 loss: {l1:0.5f} \\t L2 Loss : {l2:0.5f} \\t MAPE loss : {mape:0.5f} \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only synthetic\n",
    "syn_score = []\n",
    "for e in range(5):\n",
    "    # train a baseline predictor\n",
    "    predictor_base = GRU(input_dim=5, hidden_dim=50, output_dim=1, num_layers=2).to(device)\n",
    "    optimizer_base = Adam(predictor_base.parameters(), lr=0.001)\n",
    "    lossfn = nn.L1Loss()\n",
    "    train_model(predictor_base, \n",
    "                dl_train, \n",
    "                lossfn, \n",
    "                optimizer_base, \n",
    "                num_epochs=5000, \n",
    "                description=\"Baseline\",\n",
    "                device=device)\n",
    "    l1, l2, mape, pred_y, true_y = evaluate_model(predictor_base, dl_test, device=device)\n",
    "    print(f\"Baseline : L1 loss: {l1:0.5f} \\t L2 Loss : {l2:0.5f} \\t MAPE loss : {mape:0.5f} \")\n",
    "\n",
    "    # additional training on synthetic data\n",
    "    synthetic_data = diffusion_ts.generate_mts(batch_size=3000)\n",
    "    synthetic_data = TensorDataset(torch.from_numpy(synthetic_data))\n",
    "    dl_synthetic = DataLoader(synthetic_data, batch_size=batch_size, shuffle=True)\n",
    "    train_model(predictor_base, \n",
    "                dl_synthetic, \n",
    "                lossfn, \n",
    "                optimizer_base, \n",
    "                num_epochs=5000, \n",
    "                description=\"Synthetic\",\n",
    "                device=device)\n",
    "    l1, l2, mape, pred_y, true_y = evaluate_model(predictor_base, dl_test, device=device)\n",
    "    syn_score.append([l1, l2, mape])\n",
    "    print(f\"Synthetic : L1 loss: {l1:0.5f} \\t L2 Loss : {l2:0.5f} \\t MAPE loss : {mape:0.5f} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only origin\n",
    "origin_score = []\n",
    "for e in range(5):\n",
    "    # train a baseline predictor\n",
    "    predictor_base = GRU(input_dim=5, hidden_dim=50, output_dim=1, num_layers=2).to(device)\n",
    "    optimizer_base = Adam(predictor_base.parameters(), lr=0.001)\n",
    "    lossfn = nn.L1Loss()\n",
    "    train_model(predictor_base, \n",
    "                dl_train, \n",
    "                lossfn, \n",
    "                optimizer_base, \n",
    "                num_epochs=5000, \n",
    "                description=\"Baseline\",\n",
    "                device=device)\n",
    "    l1, l2, mape, pred_y, true_y = evaluate_model(predictor_base, dl_test, device=device)\n",
    "    print(f\"Baseline : L1 loss: {l1:0.5f} \\t L2 Loss : {l2:0.5f} \\t MAPE loss : {mape:0.5f} \")\n",
    "\n",
    "    # additional training on original data\n",
    "    idx = np.random.permutation(len(ds_train))[:3000]\n",
    "    origin_data = ds_train.data_diff[idx]\n",
    "    origin_data = TensorDataset(torch.from_numpy(origin_data))\n",
    "    dl_origin = DataLoader(origin_data, batch_size=batch_size, shuffle=True)\n",
    "    train_model(predictor_base, \n",
    "                dl_origin, \n",
    "                lossfn, \n",
    "                optimizer_base, \n",
    "                num_epochs=5000, \n",
    "                description=\"Origin\",\n",
    "                device=device)\n",
    "    l1, l2, mape, pred_y, true_y = evaluate_model(predictor_base, dl_test, device=device)\n",
    "    origin_score.append([l1, l2, mape])\n",
    "    print(f\"Origin : L1 loss: {l1:0.5f} \\t L2 Loss : {l2:0.5f} \\t MAPE loss : {mape:0.5f} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# origin+synthetic\n",
    "ori_syn_score = []\n",
    "for e in range(5):\n",
    "    # train a baseline predictor\n",
    "    predictor_base = GRU(input_dim=5, hidden_dim=50, output_dim=1, num_layers=2).to(device)\n",
    "    optimizer_base = Adam(predictor_base.parameters(), lr=0.001)\n",
    "    lossfn = nn.L1Loss()\n",
    "    train_model(predictor_base, \n",
    "                dl_train, \n",
    "                lossfn, \n",
    "                optimizer_base, \n",
    "                num_epochs=5000, \n",
    "                description=\"Baseline\",\n",
    "                device=device)\n",
    "    l1, l2, mape, pred_y, true_y = evaluate_model(predictor_base, dl_test, device=device)\n",
    "    print(f\"Baseline : L1 loss: {l1:0.5f} \\t L2 Loss : {l2:0.5f} \\t MAPE loss : {mape:0.5f} \")\n",
    "\n",
    "    # additional training on ori+syn data\n",
    "    idx = np.random.permutation(len(ds_train))[:1500]\n",
    "    origin_data = ds_train.data_diff[idx]\n",
    "    synthetic_data = diffusion_ts.generate_mts(batch_size=1500)\n",
    "    ori_syn_data = np.concatenate([origin_data, synthetic_data])\n",
    "    ori_syn_data = TensorDataset(torch.from_numpy(ori_syn_data))\n",
    "    dl_ori_syn = DataLoader(ori_syn_data, batch_size=batch_size, shuffle=True)\n",
    "    train_model(predictor_base, \n",
    "                dl_ori_syn, \n",
    "                lossfn, \n",
    "                optimizer_base, \n",
    "                num_epochs=5000, \n",
    "                description=\"Ori+Syn\",\n",
    "                device=device)\n",
    "    l1, l2, mape, pred_y, true_y = evaluate_model(predictor_base, dl_test, device=device)\n",
    "    ori_syn_score.append([l1, l2, mape])\n",
    "    print(f\"Ori+Syn : L1 loss: {l1:0.5f} \\t L2 Loss : {l2:0.5f} \\t MAPE loss : {mape:0.5f} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_syn_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without Differencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, num_epochs, description, device):\n",
    "    model.train()\n",
    "    with tqdm(range(num_epochs), total=num_epochs) as pbar:\n",
    "        for _ in pbar:\n",
    "            for data_norm, *_ in dataloader:\n",
    "                x_train = data_norm[:,:-1,:].float().to(device)\n",
    "                y_train = data_norm[:,-1:,0].float().to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(x_train)\n",
    "                loss = criterion(outputs, y_train)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            pbar.set_description(f\"{description} loss: {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    l1loss = nn.L1Loss()\n",
    "    l2loss = nn.MSELoss()\n",
    "    mapeloss = MeanAbsolutePercentageError().to(device)\n",
    "    \n",
    "    total_l1 = 0\n",
    "    total_l2 = 0\n",
    "    predictions, ground_truth = [], []\n",
    "    with torch.no_grad():\n",
    "        for data_norm, data_mean, data_std  in dataloader:\n",
    "            data_norm = data_norm.to(device)\n",
    "            data_mean = data_mean.to(device)\n",
    "            data_std = data_std.to(device)\n",
    "            batch_size = len(data_norm)\n",
    "            \n",
    "            x_test = data_norm[:, :-1, :].float()\n",
    "            y_true_norm = data_norm[:, -1:, :1].float()\n",
    "            y_pred_norm = model(x_test).view(-1,1,1)\n",
    "            \n",
    "            y_pred_unnorm = y_pred_norm * data_std[:, :, :1] + data_mean[:, :, :1]\n",
    "            y_true_unnorm = y_true_norm * data_std[:, :, :1] + data_mean[:, :, :1]\n",
    "            \n",
    "            total_l1 += l1loss(y_pred_unnorm, y_true_unnorm) * batch_size\n",
    "            total_l2 += l2loss(y_pred_unnorm, y_true_unnorm) * batch_size\n",
    "\n",
    "            predictions.append(y_pred_unnorm.cpu().numpy())\n",
    "            ground_truth.append(y_true_unnorm.cpu().numpy())\n",
    "\n",
    "    n_data = len(dataloader.dataset)\n",
    "    total_l1 /= n_data\n",
    "    total_l2 /= n_data\n",
    "    predictions = np.concatenate(predictions).squeeze()\n",
    "    ground_truth = np.concatenate(ground_truth).squeeze()\n",
    "    mape_loss = mapeloss(torch.tensor(predictions), torch.tensor(ground_truth)).item()\n",
    "    \n",
    "    return total_l1.item(), total_l2.item(), mape_loss, predictions, ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configurations\n",
    "configs = load_yaml_config(\"configs/experiments1_wo_diff.yaml\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Initialize Diffusion_TS Model\n",
    "diffusion_ts = instantiate_from_config(configs['model']).to(device)\n",
    "diffusion_ts.load_state_dict(torch.load(\"check_points/experiments1_wo_diff/DiffusionTS_5000.pth\"))\n",
    "\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataloader, dataset\n",
    "dl_info_train = dataloader_info(configs, train=True)\n",
    "dl_info_test = dataloader_info(configs, train=False)\n",
    "\n",
    "dl_train = dl_info_train[\"dataloader\"]\n",
    "ds_train = dl_info_train[\"dataset\"]\n",
    "\n",
    "dl_test = dl_info_test[\"dataloader\"]\n",
    "ds_test = dl_info_test[\"dataset\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. pre-training for baseline predictor\n",
    "predictor_base = GRU(input_dim=5, \n",
    "                   hidden_dim=50, \n",
    "                   output_dim=1, \n",
    "                   num_layers=2).to(device)\n",
    "optimizer_base = Adam(predictor_base.parameters(), lr=0.001)\n",
    "lossfn = nn.L1Loss()\n",
    "\n",
    "train_model(predictor_base, \n",
    "            dl_train, \n",
    "            lossfn, \n",
    "            optimizer_base, \n",
    "            num_epochs=3000, \n",
    "            description=\"Baseline\",\n",
    "            device=device)\n",
    "\n",
    "l1, l2, mape, pred_y, true_y = evaluate_model(predictor_base, dl_test, device=device)\n",
    "print(f\"Baseline : L1 loss: {l1:0.5f} \\t L2 Loss : {l2:0.5f} \\t MAPE loss : {mape:0.5f} \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Baseline loss: 0.017479: 100%|██████████| 5000/5000 [02:28<00:00, 33.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline : L1 loss: 1.33639 \t L2 Loss : 4.95665 \t MAPE loss : 0.01685 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reverse step from x_T to x_0:   0%|          | 0/100 [00:00<?, ?it/s]/home/harim/Desktop/pyproject/Project_TS_Generation/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "reverse step from x_T to x_0: 100%|██████████| 100/100 [01:05<00:00,  1.53it/s]\n",
      "Synthetic loss: 0.019662: 100%|██████████| 5000/5000 [01:49<00:00, 45.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic : L1 loss: 1.36002 \t L2 Loss : 5.15284 \t MAPE loss : 0.01709 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Baseline loss: 0.180678:   3%|▎         | 152/5000 [00:04<02:23, 33.87it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m optimizer_base \u001b[38;5;241m=\u001b[39m Adam(predictor_base\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m      7\u001b[0m lossfn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mL1Loss()\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictor_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlossfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptimizer_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBaseline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m l1, l2, mape, pred_y, true_y \u001b[38;5;241m=\u001b[39m evaluate_model(predictor_base, dl_test, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseline : L1 loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m L2 Loss : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml2\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m MAPE loss : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmape\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, criterion, optimizer, num_epochs, description, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_train)\n\u001b[1;32m     11\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 12\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdescription\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/pyproject/Project_TS_Generation/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/pyproject/Project_TS_Generation/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/Desktop/pyproject/Project_TS_Generation/.venv/lib/python3.12/site-packages/torch/optim/adam.py:187\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    160\u001b[0m         group,\n\u001b[1;32m    161\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    166\u001b[0m         state_steps)\n\u001b[1;32m    168\u001b[0m     adam(\n\u001b[1;32m    169\u001b[0m         params_with_grad,\n\u001b[1;32m    170\u001b[0m         grads,\n\u001b[1;32m    171\u001b[0m         exp_avgs,\n\u001b[1;32m    172\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    173\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    174\u001b[0m         state_steps,\n\u001b[1;32m    175\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    176\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    177\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    178\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    179\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    180\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    181\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    182\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    183\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    184\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    185\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    186\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m--> 187\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m,\n\u001b[1;32m    188\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    189\u001b[0m     )\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# only synthetic\n",
    "syn_score = []\n",
    "for e in range(5):\n",
    "    # train a baseline predictor\n",
    "    predictor_base = GRU(input_dim=5, hidden_dim=50, output_dim=1, num_layers=2).to(device)\n",
    "    optimizer_base = Adam(predictor_base.parameters(), lr=0.001)\n",
    "    lossfn = nn.L1Loss()\n",
    "    train_model(predictor_base, \n",
    "                dl_train, \n",
    "                lossfn, \n",
    "                optimizer_base, \n",
    "                num_epochs=5000, \n",
    "                description=\"Baseline\",\n",
    "                device=device)\n",
    "    l1, l2, mape, pred_y, true_y = evaluate_model(predictor_base, dl_test, device=device)\n",
    "    print(f\"Baseline : L1 loss: {l1:0.5f} \\t L2 Loss : {l2:0.5f} \\t MAPE loss : {mape:0.5f} \")\n",
    "\n",
    "    # additional training on synthetic data\n",
    "    synthetic_data = diffusion_ts.generate_mts(batch_size=3000)\n",
    "    synthetic_data = TensorDataset(torch.from_numpy(synthetic_data))\n",
    "    dl_synthetic = DataLoader(synthetic_data, batch_size=batch_size, shuffle=True)\n",
    "    train_model(predictor_base, \n",
    "                dl_synthetic, \n",
    "                lossfn, \n",
    "                optimizer_base, \n",
    "                num_epochs=5000, \n",
    "                description=\"Synthetic\",\n",
    "                device=device)\n",
    "    l1, l2, mape, pred_y, true_y = evaluate_model(predictor_base, dl_test, device=device)\n",
    "    syn_score.append([l1, l2, mape])\n",
    "    print(f\"Synthetic : L1 loss: {l1:0.5f} \\t L2 Loss : {l2:0.5f} \\t MAPE loss : {mape:0.5f} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Baseline loss: 0.021086:  90%|█████████ | 4516/5000 [02:12<00:14, 34.05it/s]"
     ]
    }
   ],
   "source": [
    "# only origin\n",
    "origin_score = []\n",
    "for e in range(5):\n",
    "    # train a baseline predictor\n",
    "    predictor_base = GRU(input_dim=5, hidden_dim=50, output_dim=1, num_layers=2).to(device)\n",
    "    optimizer_base = Adam(predictor_base.parameters(), lr=0.001)\n",
    "    lossfn = nn.L1Loss()\n",
    "    train_model(predictor_base, \n",
    "                dl_train, \n",
    "                lossfn, \n",
    "                optimizer_base, \n",
    "                num_epochs=5000, \n",
    "                description=\"Baseline\",\n",
    "                device=device)\n",
    "    l1, l2, mape, pred_y, true_y = evaluate_model(predictor_base, dl_test, device=device)\n",
    "    print(f\"Baseline : L1 loss: {l1:0.5f} \\t L2 Loss : {l2:0.5f} \\t MAPE loss : {mape:0.5f} \")\n",
    "\n",
    "    # additional training on original data\n",
    "    idx = np.random.permutation(len(ds_train))[:3000]\n",
    "    origin_data = ds_train.data_diff[idx]\n",
    "    origin_data = TensorDataset(torch.from_numpy(origin_data))\n",
    "    dl_origin = DataLoader(origin_data, batch_size=batch_size, shuffle=True)\n",
    "    train_model(predictor_base, \n",
    "                dl_origin, \n",
    "                lossfn, \n",
    "                optimizer_base, \n",
    "                num_epochs=5000, \n",
    "                description=\"Origin\",\n",
    "                device=device)\n",
    "    l1, l2, mape, pred_y, true_y = evaluate_model(predictor_base, dl_test, device=device)\n",
    "    origin_score.append([l1, l2, mape])\n",
    "    print(f\"Origin : L1 loss: {l1:0.5f} \\t L2 Loss : {l2:0.5f} \\t MAPE loss : {mape:0.5f} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# origin+synthetic\n",
    "ori_syn_score = []\n",
    "for e in range(5):\n",
    "    # train a baseline predictor\n",
    "    predictor_base = GRU(input_dim=5, hidden_dim=50, output_dim=1, num_layers=2).to(device)\n",
    "    optimizer_base = Adam(predictor_base.parameters(), lr=0.001)\n",
    "    lossfn = nn.L1Loss()\n",
    "    train_model(predictor_base, \n",
    "                dl_train, \n",
    "                lossfn, \n",
    "                optimizer_base, \n",
    "                num_epochs=5000, \n",
    "                description=\"Baseline\",\n",
    "                device=device)\n",
    "    l1, l2, mape, pred_y, true_y = evaluate_model(predictor_base, dl_test, device=device)\n",
    "    print(f\"Baseline : L1 loss: {l1:0.5f} \\t L2 Loss : {l2:0.5f} \\t MAPE loss : {mape:0.5f} \")\n",
    "\n",
    "    # additional training on ori+syn data\n",
    "    idx = np.random.permutation(len(ds_train))[:1500]\n",
    "    origin_data = ds_train.data_diff[idx]\n",
    "    synthetic_data = diffusion_ts.generate_mts(batch_size=1500)\n",
    "    ori_syn_data = np.concatenate([origin_data, synthetic_data])\n",
    "    ori_syn_data = TensorDataset(torch.from_numpy(ori_syn_data))\n",
    "    dl_ori_syn = DataLoader(ori_syn_data, batch_size=batch_size, shuffle=True)\n",
    "    train_model(predictor_base, \n",
    "                dl_ori_syn, \n",
    "                lossfn, \n",
    "                optimizer_base, \n",
    "                num_epochs=5000, \n",
    "                description=\"Ori+Syn\",\n",
    "                device=device)\n",
    "    l1, l2, mape, pred_y, true_y = evaluate_model(predictor_base, dl_test, device=device)\n",
    "    ori_syn_score.append([l1, l2, mape])\n",
    "    print(f\"Ori+Syn : L1 loss: {l1:0.5f} \\t L2 Loss : {l2:0.5f} \\t MAPE loss : {mape:0.5f} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_syn_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
