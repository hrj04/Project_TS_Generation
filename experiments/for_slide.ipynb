{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harim/Desktop/pyproject/Project_TS_Generation/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics import MeanAbsolutePercentageError\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from data.dataloader import dl_from_numpy, dataloader_info\n",
    "from models.predictor import GRU\n",
    "from utils.utils import load_yaml_config, instantiate_from_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configurations\n",
    "configs = load_yaml_config(\"configs/stock_prediction.yaml\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load dataset info\n",
    "dl_info_train = dataloader_info(configs)\n",
    "dl_info_test = dataloader_info(configs, train=False)\n",
    "dataset = dl_info_train['dataset']\n",
    "seq_length, feature_dim = dataset.window, dataset.feature_dim\n",
    "batch_size = configs[\"dataloader\"][\"batch_size\"]\n",
    "\n",
    "\n",
    "# dataset\n",
    "test_data_norm = torch.from_numpy(np.load(os.path.join(dataset.dir, f\"stock_ground_truth_data_{seq_length}_test.npy\"))).to(device)\n",
    "test_mean = torch.from_numpy(np.load(os.path.join(dataset.dir, f\"stock_ground_truth_mean_{seq_length}_test.npy\"))).to(device)\n",
    "test_std = torch.from_numpy(np.load(os.path.join(dataset.dir, f\"stock_ground_truth_std_{seq_length}_test.npy\"))).to(device)\n",
    "test_dataset = TensorDataset(test_data_norm, test_mean, test_std)\n",
    "# fake_dataset = torch.from_numpy(np.load(os.path.join(dataset.dir, f\"ddpm_fake_stock.npy\"))).to(device)\n",
    "# ori_fake_dataset = TensorDataset(dl_info_train[\"dataset\"], fake_dataset)\n",
    "\n",
    "# load dataloader\n",
    "ori_dl = dl_info_train[\"dataloader\"]\n",
    "fake_dl = dl_from_numpy(os.path.join(dataset.dir, f\"ddpm_fake_stock.npy\"), batch_size=batch_size)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "# ori_fake_dl = DataLoader(ori_fake_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "lossfn = nn.L1Loss()\n",
    "lr = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=100, description=\"\"):\n",
    "    model.train()\n",
    "    with tqdm(range(num_epochs), total=num_epochs) as pbar:\n",
    "        for e in pbar:\n",
    "            for data in dataloader:\n",
    "                x_train = data[:,:-1,:].float().to(device)\n",
    "                y_train = data[:,-1:,0].float().to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(x_train)\n",
    "                loss = criterion(outputs, y_train)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            pbar.set_description(f\"{description} loss: {loss.item():.6f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    # define loss for comparison\n",
    "    l1loss = nn.L1Loss()\n",
    "    l2loss = nn.MSELoss()\n",
    "    mapeloss = MeanAbsolutePercentageError().to(device)\n",
    "    \n",
    "    total_l1 = 0\n",
    "    total_l2 = 0\n",
    "    total_mape = 0\n",
    "\n",
    "    predictions, true_vals = [], []\n",
    "    with torch.no_grad():\n",
    "        for data, mean, std in dataloader:\n",
    "            x_test = data[:, :(seq_length - 1), :].float().to(device)\n",
    "            y_test = data[:, (seq_length - 1):, :1].float().to(device)\n",
    "            mean = mean[:, :, :1].float().to(device)\n",
    "            std = std[:, :, :1].float().to(device)\n",
    "            y_pred = model(x_test).view(-1,1,1)\n",
    "            \n",
    "            total_l1 += l1loss(y_pred, y_test) * len(data)\n",
    "            total_l2 += l2loss(y_pred, y_test) * len(data)\n",
    "            total_mape += mapeloss(y_pred, y_test).item()\n",
    "\n",
    "            y_test_unnorm = y_test * std + mean\n",
    "            y_pred_unnorm = y_pred * std + mean\n",
    "\n",
    "            predictions.append(y_pred_unnorm.cpu().numpy())\n",
    "            true_vals.append(y_test_unnorm.cpu().numpy())\n",
    "\n",
    "    n_data = len(dataloader.dataset)\n",
    "    total_l1 /= n_data\n",
    "    total_l2 /= n_data\n",
    "    total_mape /= n_data\n",
    "    \n",
    "    predictions = np.concatenate(predictions).squeeze()\n",
    "    true_vals = np.concatenate(true_vals).squeeze()\n",
    "    # mape_loss = mapeloss(torch.tensor(predictions), torch.tensor(true_vals)).item()\n",
    "    \n",
    "    return total_l1, total_l2, total_mape, predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Diffusion_TS Model\n",
    "diffusion_adversarial = instantiate_from_config(configs['model']).to(device)\n",
    "diffusion_adversarial.load_state_dict(torch.load(\"check_points/stock_24/model_50000.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Original loss: 0.027205: 100%|██████████| 2000/2000 [00:46<00:00, 42.91it/s]\n",
      "  0%|          | 0/27 [00:00<?, ?it/s]/home/harim/Desktop/pyproject/Project_TS_Generation/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "100%|██████████| 27/27 [00:40<00:00,  1.51s/it]\n",
      "SynAdv loss: 0.016951: 100%|██████████| 2000/2000 [00:45<00:00, 44.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adv_synthetic : L1 loss: 0.59515 \t L2 Loss : 0.66888 \t MAPE loss : 0.01390 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Original loss: 0.027459: 100%|██████████| 2000/2000 [00:49<00:00, 40.02it/s]\n",
      "100%|██████████| 27/27 [00:43<00:00,  1.59s/it]\n",
      "SynAdv loss: 0.019030: 100%|██████████| 2000/2000 [00:46<00:00, 42.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adv_synthetic : L1 loss: 0.59762 \t L2 Loss : 0.68249 \t MAPE loss : 0.01258 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Original loss: 0.025052: 100%|██████████| 2000/2000 [00:59<00:00, 33.68it/s]\n",
      "100%|██████████| 27/27 [00:42<00:00,  1.58s/it]\n",
      "SynAdv loss: 0.020690: 100%|██████████| 2000/2000 [00:47<00:00, 42.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adv_synthetic : L1 loss: 0.59301 \t L2 Loss : 0.67077 \t MAPE loss : 0.01461 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Original loss: 0.026376: 100%|██████████| 2000/2000 [01:00<00:00, 33.15it/s]\n",
      "100%|██████████| 27/27 [00:40<00:00,  1.51s/it]\n",
      "SynAdv loss: 0.021582: 100%|██████████| 2000/2000 [00:45<00:00, 44.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adv_synthetic : L1 loss: 0.60828 \t L2 Loss : 0.68751 \t MAPE loss : 0.01471 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Original loss: 0.025227: 100%|██████████| 2000/2000 [00:47<00:00, 41.79it/s]\n",
      "100%|██████████| 27/27 [00:42<00:00,  1.57s/it]\n",
      "SynAdv loss: 0.018578: 100%|██████████| 2000/2000 [00:45<00:00, 44.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adv_synthetic : L1 loss: 0.61220 \t L2 Loss : 0.69547 \t MAPE loss : 0.01543 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train on original + adv_synthetic data\n",
    "synadv_ori_l1_list = []\n",
    "synadv_ori_l2_list = []\n",
    "synadv_ori_mape_list = []\n",
    "ori_dataset = dl_info_train[\"dataset\"].data.to(device)\n",
    "\n",
    "e = 2000\n",
    "for _ in range(5):\n",
    "    model_ori = GRU(input_dim=feature_dim, hidden_dim=50, num_layers=2).to(device)\n",
    "    optimizer_ori = optim.Adam(model_ori.parameters(), lr=lr)\n",
    "    \n",
    "    # train original model on original data\n",
    "    train_model(model_ori, ori_dl, lossfn, optimizer_ori, num_epochs=e, description=\"Original\")\n",
    "    \n",
    "    # make adv_data on trained original model\n",
    "    adv_data = diffusion_adversarial.generate_adversarial(ori_dataset, predictor=model_ori, batch_size=128, num_timesteps=10)\n",
    "    adv_data = adv_data.to(device)\n",
    "    ori_adv_dl = DataLoader(adv_data, batch_size=128, shuffle=True)\n",
    "    \n",
    "    # retrain original model on synthetic adversarial data\n",
    "    train_model(model_ori, ori_adv_dl, lossfn, optimizer_ori, num_epochs=e, description=\"SynAdv\")\n",
    "\n",
    "    synadv_ori_l1, synadv_ori_l2, synadv_ori_mape, synadv_ori_pred_y, _ = evaluate_model(model_ori, test_dl)\n",
    "    synadv_ori_l1_list.append(synadv_ori_l1.item())\n",
    "    synadv_ori_l2_list.append(synadv_ori_l2.item())\n",
    "    synadv_ori_mape_list.append(synadv_ori_mape)\n",
    "    print(f\"Adv_synthetic : L1 loss: {synadv_ori_l1:0.5f} \\t L2 Loss : {synadv_ori_l2:0.5f} \\t MAPE loss : {synadv_ori_mape:0.5f} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synadv_ori_l1 : \t mean : 0.601 \t std : 0.008\n",
      "synadv_ori_l2 : \t mean : 0.681 \t std : 0.010\n",
      "synadv_ori_mape : \t mean : 0.014 \t std : 0.001\n"
     ]
    }
   ],
   "source": [
    "print(f\"synadv_ori_l1 : \\t mean : {np.mean(synadv_ori_l1_list):0.4f} \\t std : {np.std(synadv_ori_l1_list):0.4f}\")\n",
    "print(f\"synadv_ori_l2 : \\t mean : {np.mean(synadv_ori_l2_list):0.4f} \\t std : {np.std(synadv_ori_l2_list):0.4f}\")\n",
    "print(f\"synadv_ori_mape : \\t mean : {np.mean(synadv_ori_mape_list):0.4f} \\t std : {np.std(synadv_ori_mape_list):0.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Original loss: 0.019538: 100%|██████████| 4000/4000 [01:34<00:00, 42.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original : L1 loss: 0.59797 \t L2 Loss : 0.67639 \t MAPE loss : 0.01425 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Original loss: 0.019888: 100%|██████████| 4000/4000 [01:36<00:00, 41.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original : L1 loss: 0.61059 \t L2 Loss : 0.71364 \t MAPE loss : 0.01540 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Original loss: 0.023438: 100%|██████████| 4000/4000 [01:50<00:00, 36.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original : L1 loss: 0.62171 \t L2 Loss : 0.72752 \t MAPE loss : 0.01526 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Original loss: 0.020708: 100%|██████████| 4000/4000 [01:33<00:00, 42.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original : L1 loss: 0.60606 \t L2 Loss : 0.71461 \t MAPE loss : 0.01411 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Original loss: 0.023456:  62%|██████▏   | 2485/4000 [00:58<00:35, 42.67it/s]"
     ]
    }
   ],
   "source": [
    "# Train on original data\n",
    "ori_l1_list = []\n",
    "ori_l2_list = []\n",
    "ori_mape_list = []\n",
    "e = 4000\n",
    "\n",
    "for _ in range(5):\n",
    "    model_ori = GRU(input_dim=feature_dim, hidden_dim=50, num_layers=2).to(device)\n",
    "    optimizer_ori = optim.Adam(model_ori.parameters(), lr=lr)\n",
    "    train_model(model_ori, ori_dl, lossfn, optimizer_ori, num_epochs=e, description=\"Original\")\n",
    "    \n",
    "    ori_l1, ori_l2, ori_mape, ori_pred_y, true_y = evaluate_model(model_ori, test_dl)\n",
    "    ori_l1_list.append(ori_l1.item())\n",
    "    ori_l2_list.append(ori_l2.item())\n",
    "    ori_mape_list.append(ori_mape)\n",
    "    print(f\"Original : L1 loss: {ori_l1:0.5f} \\t L2 Loss : {ori_l2:0.5f} \\t MAPE loss : {ori_mape:0.5f} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ori_l1 : \\t mean : {np.mean(ori_l1_list):0.4f} \\t std : {np.std(ori_l1_list):0.4f}\")\n",
    "print(f\"ori_l2 : \\t mean : {np.mean(ori_l2_list):0.4f} \\t std : {np.std(ori_l2_list):0.4f}\")\n",
    "print(f\"ori_mape : \\t mean : {np.mean(ori_mape_list):0.4f} \\t std : {np.std(ori_mape_list):0.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on Synthetic data\n",
    "syn_l1_list = []\n",
    "syn_l2_list = []\n",
    "syn_mape_list = []\n",
    "e = 4000\n",
    "for _ in range(5):\n",
    "    model_syn = GRU(input_dim=feature_dim, hidden_dim=50, num_layers=2).to(device)\n",
    "    optimizer_syn = optim.Adam(model_syn.parameters(), lr=lr)\n",
    "    train_model(model_syn, fake_dl, lossfn, optimizer_syn, num_epochs=e, description=\"Synthetic\")\n",
    "    \n",
    "    syn_l1, syn_l2, syn_mape, syn_pred_y, _ = evaluate_model(model_syn, test_dl)\n",
    "    syn_l1_list.append(syn_l1.item())\n",
    "    syn_l2_list.append(syn_l2.item())\n",
    "    syn_mape_list.append(syn_mape)\n",
    "    print(f\"Synthetic : L1 loss: {syn_l1:0.5f} \\t L2 Loss : {syn_l2:0.5f} \\t MAPE loss : {syn_mape:0.5f} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"syn_l1 : \\t mean : {np.mean(syn_l1_list):0.4f} \\t std : {np.std(syn_l1_list):0.4f}\")\n",
    "print(f\"syn_l2 : \\t mean : {np.mean(syn_l2_list):0.4f} \\t std : {np.std(syn_l2_list):0.4f}\")\n",
    "print(f\"syn_mape : \\t mean : {np.mean(syn_mape_list):0.4f} \\t std : {np.std(syn_mape_list):0.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_train_model(model, dataloader, criterion, optimizer, epsilon=0.01, num_epochs=100, description=\"\"):\n",
    "    model.train()\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    adv_data = []\n",
    "    with tqdm(range(num_epochs), total=num_epochs) as pbar:\n",
    "        for e in pbar:\n",
    "            running_loss = 0.0\n",
    "            for data in dataloader:\n",
    "                x_train = data[:, :-1, :].float().to(device)\n",
    "                y_train = data[:, -1:, 0].float().to(device)\n",
    "                x_train.requires_grad = True\n",
    "                \n",
    "                # Standard training forward pass\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(x_train)\n",
    "                loss = criterion(outputs, y_train)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Generate adversarial examples\n",
    "                grad = x_train.grad.data\n",
    "                x_adv = (x_train + epsilon * grad.sign()).clamp(-4, 4).detach()\n",
    "                \n",
    "                if e+1 == num_epochs: \n",
    "                    adv_data.append(x_adv.cpu())\n",
    "\n",
    "                # Adversarial training forward pass\n",
    "                x_adv.requires_grad = False\n",
    "                outputs_adv = model(x_adv)\n",
    "                loss_adv = criterion(outputs_adv, y_train)\n",
    "                loss_adv.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss_adv.item()\n",
    "            \n",
    "            avg_loss = running_loss / len(dataloader)\n",
    "            pbar.set_description(f\"{description} loss: {avg_loss:.6f}\")\n",
    "            \n",
    "    return adv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on original + adv_original data\n",
    "adv_ori_l1_list = []\n",
    "adv_ori_l2_list = []\n",
    "adv_ori_mape_list = []\n",
    "e = 2000\n",
    "epsilon = 0.1\n",
    "\n",
    "for _ in range(5):\n",
    "    model_ori = GRU(input_dim=feature_dim, hidden_dim=50, num_layers=2).to(device)\n",
    "    optimizer_ori = optim.Adam(model_ori.parameters(), lr=lr)\n",
    "    train_model(model_ori, ori_dl, lossfn, optimizer_ori, num_epochs=e, description=\"Original\")\n",
    "    adv_ori_data = adv_train_model(model_ori, ori_dl, lossfn, optimizer_ori, epsilon=epsilon, num_epochs=e, description=\"Adv_Original\")\n",
    "\n",
    "    adv_ori_l1, adv_ori_l2, adv_ori_mape, adv_ori_pred_y, _ = evaluate_model(model_ori, test_dl)\n",
    "    adv_ori_l1_list.append(adv_ori_l1.item())\n",
    "    adv_ori_l2_list.append(adv_ori_l2.item())\n",
    "    adv_ori_mape_list.append(adv_ori_mape)\n",
    "    print(f\"Adv_Original : L1 loss: {adv_ori_l1:0.5f} \\t L2 Loss : {adv_ori_l2:0.5f} \\t MAPE loss : {adv_ori_mape:0.5f} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"adv_ori_l1 : \\t mean : {np.mean(adv_ori_l1_list):0.4f} \\t std : {np.std(adv_ori_l1_list):0.4f}\")\n",
    "print(f\"adv_ori_l2 : \\t mean : {np.mean(adv_ori_l2_list):0.4f} \\t std : {np.std(adv_ori_l2_list):0.4f}\")\n",
    "print(f\"adv_ori_mape : \\t mean : {np.mean(adv_ori_mape_list):0.4f} \\t std : {np.std(adv_ori_mape_list):0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
